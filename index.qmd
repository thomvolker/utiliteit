---
title: "Het evalueren van de kwaliteit van synthetische data"
author: "Thom Benjamin Volker <br> [t.b.volker@uu.nl](mailto:t.b.volker@uu.nl)"
format: 
  revealjs:
    slide-number: true
    df-print: kable
    html-math-method: katex
    theme: dark
---

```{r}
#| include: false

# Packages
library(ggplot2)
```


# Stel je voor dat we toegang hebben tot alle data in de wereld

_Dat zou een privacy-ramp zijn..._

::: {.notes}

Stel je voor dat we toegang hebben tot alle data in de wereld. Man, wat zou dat een privacy-ramp zijn... Gelukkig weten jullie dat er goede manieren zijn om data te delen, zonder direct de privacy van de respondenten te schenden. Synthetische data is natuurlijk een zo'n oplossing. Het probleem is alleen dat synthetische data genereren heel erg makkelijk is, maar het genereren van _goede_ synthetische data best wel lastig kan zijn. Vandaag wil ik niet zozeer stilstaan bij hoe je goede synthetische data maakt, maar wil ik jullie toch helpen bij het maken van hoge kwaliteit synthetische data, door te laten zien hoe je de kwaliteit van synthetische data kan evalueren. De uitkomsten hiervan kan je gebruiken om je synthetische data model bij te schaven. 

:::



## Wie ben ik?

<br>

Thom Volker ([t.b.volker@uu.nl](mailto:t.b.volker@uu.nl))

:::: {.columns}

::: {.column width="40%"}
![](files/me_square.jpg)
:::

::: {.column width="60%"}

- MSc. in Methoden en Statistiek & Sociologie

- PhD kandidaat bij Universiteit Utrecht en CBS

  - Doel: Doorontwikkelen van veilige (!) data synthesis technieken

:::

::::


## Open materialen

Deze presentatie staat online op

[https://thomvolker.github.io/utiliteit](https://thomvolker.github.io/utiliteit)

De broncode en resultaten zijn te vinden op

[https://github.com/thomvolker/utiliteit](https://github.com/thomvolker/utiliteit)

::: aside

Deze presentatie is mede tot stand gekomen dankzij bijdragen van Gerko Vink, Stef van Buuren, Erik-Jan van Kesteren en Peter-Paul de Wolf.

:::

# Synthetische data

_Neppe data, gegenereerde data, gesimuleerde data, digital twins_

::: {.notes}
In tegenstelling tot echte, verzamelde data, die voortkomt uit de complexe systemen in deze wereld.
:::



## Synthetische data generatie cyclus

1. Creëer synthetische data met simpele/algemene modellen

2. Evalueer of de synthetische data de gewenste kwaliteit heeft

3. Indien nodig, voeg complexiteit toe (specifieke modellen, transformations, interacties)

4. Itereer tussen (2.) en (3.) totdat de synthetische data naar wens is


# De privacy-utility trade-off

Synthetische data is altijd een compromis tussen het beschermen van privacy en het behouden van utiliteit


## Privacy en utiliteit zijn tegenpolen

<br>

![](files/pu_arrow.jpg)

<br>

De vraag is: hoeveel informatie moeten we opofferen om de privacy van de respondenten te beschermen?

Of: welk privacy risico is aanvaardbaar om een zo bruikbaar mogelijke dataset te behouden?


## Privacy versus utiliteit

- Elke parameter in het model bevat informatie over de observaties in de echte data

- Hoe meer parameters je gebruikt om synthetische data te generen, hoe hoger de utiliteit

- Wanneer de informatie in de parameters gelijk is aan de informatie in de echte data, recreëren we de echte data

- Op dat moment is er geen privacybescherming meer

## Privacy versus utiliteit


```{r}
#| echo: false
set.seed(9)
n <- 10
x <- seq(0, 4, length.out = n)
y <- sin(x) + rnorm(n, 0, 0.2)

fit1  <- lm(y ~ x)
pred1 <- predict(fit1, interval = "p")
fit2x <- data.frame(poly(x, 9, raw = TRUE)) |> as.matrix()
fit2  <- lm(y ~ fit2x)
predx <- seq(0, 4, length.out = 1000) |> poly(9, raw = TRUE)
pred2 <- cbind(1, predx) %*% coef(fit2)

ggplot(NULL, aes(x, y)) +
  geom_point() +
  geom_ribbon(aes(ymin = pred1[,2], ymax = pred1[,3], 
                  col = "Underparameterized"), alpha = 0.1) +
  geom_abline(col = RColorBrewer::brewer.pal(3, "Set1")[2], 
              slope = fit1$coefficients[2], intercept = fit1$coefficients[1]) +
  geom_line(aes(x = seq(0, 4, length.out = 1000), y = pred2[,1], 
                col = "Overparameterized",
                fill = "Overparameterized")) +
  geom_function(aes(x = NULL, y = NULL), fun = sin) +
  theme_minimal() +
  scale_color_brewer(name = "Model", palette = "Set1")
```

# Wanneer is synthetische data bruikbaar?

::: {.notes}

Synthetische data is bruikbaar als het goed genoeg is voor het doel waartoe het dient. 
Kan je verwachten dat je elke mogelijke analyse die je had kunnen doen op de echte data, ook kan doen op de synthetische data? Dat zou fantastisch zijn, maar waarschijnlijk is dat niet de realiteit, en dat hoeft ook niet per se. Desalniettemin zou het mooi zijn om wel zo dicht mogelijk bij dit ideaal te komen. Toch zou het mooi zijn als we zo dicht mogelijk bij dit ideaal kunnen komen, omdat de synthetische data dan het meest tot zijn recht komt, de echte data het minst gebruikt hoeft te worden, en dus het best beschermd wordt. Het evalueren van utiliteit kan helpen om de synthetische data zo goed mogelijk te maken. Uiteraard allemaal binnen de grenzen van een acceptabel privacy risico.

:::

##

### Intuïtief

- Kunnen we de synthetische data voor dezelfde doeleinden gebruiken als de geobserveerde data?

- Hebben de synthetische en geobserveerde data een vergelijkbare verdeling?


### Praktisch

- Geven analyses van de synthetische en geobserveerde data vergelijkbare resultaten?

- Kunnen we de synthetische data onderscheiden van de echte data?


::: {.notes}

Er zijn verschillende manieren om dit te beoordelen, en ik wil jullie nu even stap-voor-stap meenemen door een aantal klassen van utiliteitsmetrieken. 

:::


## Kwaliteitsmaten voor synthetische data

<br>

__Fit-for-purpose utiliteitsmaten__

<br>

__Analyse-specifieke utiliteitsmaten__

<br>

__Algemene / globale utiliteitsmaten__

## Fit-for-purpose maten

<br>

::: {.callout-tip title="Startpunt van utiliteitsevaluatie"}

De synthetische data moet geschikt zijn voor het doel waartoe het dient

:::

<br>

Juiste variabele-typen: zijn categorische variabelen nog categorische variabelen?

Plausibiliteit: negatieve leeftijden, peuters met een masterdiploma

__Visuele inspectie (vaak de belangrijkste methode)__

::: {.notes}

Hoewel het allemaal heel simpel klinkt, is het in mijn ogen het belangrijkste aspect van het beoordelen van de kwaliteit van synthetische data. Als er op het meest basale niveau verschillen zitten, is de kans groot dat andere aspecten ook niet kloppen. 
Uiteraard, als de variabelen-typen niet kloppen (i.e., een categorische variabele is een numerieke variabele zonder labels geworden), is het moeilijk om de data te gebruiken. Kunnen de goede analyses nog wel uitgevoerd worden?

Hetzelfde geldt voor plausibiliteit. Wanneer de data negatieve leeftijden bevat, peuters met een PhD

:::


## Voorbeeld

<br>

Is dit "plausibele" synthetische data?

<br>

```{r}
library(ggplot2)
set.seed(123)
n <- 500
obs <- rmultinom(n, 1, rep(1/5, 5)) |> apply(2, which.max)
syn <- rnorm(n, 3, sqrt(2))

psych::describe(data.frame(Observed  = obs, Synthetic = syn)) |>
  dplyr::select(2, 3, 4, 5, 11)
```

## Het belang van visuele inspectie

Want kwantitatieve maten kunnen misleidend zijn

```{r}
data.frame(X = c(obs, syn),
           Data = rep(c("Observed", "Synthetic"), each = n)) |> 
  ggplot(aes(X, fill = Data)) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("lightblue", "hotpink3")) +
  ggdark::dark_mode()
```

# Is dit een probleem?

::: {.notes}

Misschien. Het hangt af van het doel van de synthetische data, maar in sommige gevallen maakt de precieze verdeling van de echte data helemaal niet zoveel uit. Dat wil zeggen, voor een hoop analyses is het niet zo belangrijk dat de verdeling van de synthetische data klopt.

:::

## Analyse-specifieke utiliteitsmaten

Als je weet welke analyse uitgevoerd gaat worden, kun je synthetische data evalueren voor dit doel

<br>

```{r}
CIs <- data.frame(Mean = c(mean(obs), mean(syn))) |>
  cbind(sapply(list(obs, syn), \(x) lm(x ~ 1) |> confint()) |> t())

rownames(CIs) <- c("Observed", "Synthetic")
colnames(CIs) <- c("Gemiddelde", "2.5%", "97.5%")

CIs |> round(3)
```

```{r}
CI_o <- ((min(CIs$`97.5%`) - max(CIs$`2.5%`)) / (CIs$`97.5%`[1] - CIs$`2.5%`[1]) +
  (min(CIs$`97.5%`) - max(CIs$`2.5%`)) / (CIs$`97.5%`[2] - CIs$`2.5%`[2]))/2
```

<br>

__Betrouwbaarheidsinterval overlap:__ $`r round(CI_o, 3)`$

## Analyse-specifieke utiliteitsmaten

Als je weet dat de gebruiker een lineair regressiemodel wil schatten:

- Evalueer gemiddeldes en covariantiematrix; als deze vergelijkbaar zijn, geeft je synthetische data vergelijkbare resultaten

Als je weet dat de gebruiker een predictiemodel wil toepassen: 

- Evalueer voorspellende prestaties in de synthetische data

## Bewijs dat covariantiematrix en gemiddelden genoeg zijn

```{r}
#| echo: true
N <- 1000
P <- 5
S <- diag(P) + 1
X <- rnorm(N*P) |> matrix(N) %*% chol(S)
Y <- X %*% c(0:4/5) + rnorm(N)

coef(lm(Y ~ X))

Vobs <- var(cbind(X, Y))
b <- solve(Vobs[1:5, 1:5]) %*% Vobs[1:5, 6]
b0 <- mean(Y) - colMeans(X) %*% b
c(b0, b)
```

# Maar hoe weten we welke analyses met de synthetische data uitgevoerd worden?

::: {.notes}

Soms staat van tevoren al heel duidelijk vast welke analyses uitgevoerd gaan worden met de synthetische data, bijvoorbeeld wanneer, in het kader van open science, synthetische data wordt gedeeld bij een paper om de analyses te kunnen repliceren. Of wannneer jullie, bij DUO bijvoorbeeld, al een heel gedetailleerd onderzoeksplan krijgen van een onderzoeker die met jullie synthetische data. Maar in heel veel gevallen is het niet zo duidelijk welke analyses er precies met de synthetische data uitgevoerd gaan worden, of willen mensen toch nog aanvullende analyses met de synthetische data doen. In zulke gevallen is het belangrijk dat de synthetische data in zijn geheel lijkt op de echte data. En om dit te evalueren hebben we algemene, of globale utiliteitsmaten.

:::


## Globale utiliteitsmaten

__Als de synthetische en de geobserveerde data gelijke verdelingen hebben, zouden ze vergelijkbare resultaten moeten geven__

```{r}
data.frame(X = c(obs, syn),
           Data = rep(c("Observed", "Synthetic"), each = n)) |> 
  ggplot(aes(X, fill = Data)) +
  geom_density(alpha = 0.5) +
  scale_fill_manual(values = c("lightblue", "hotpink3")) +
  ggdark::dark_mode()
```


## Bestaande kwaliteitsmaten: $pMSE$

1. Plak synthetische en geobserveerde data onder elkaar

2. Voorspel voor elke observatie de kans $\pi_i$ dat deze synthetisch is

3. Bereken $pMSE$ als $\sum^N_{i=1} (\pi_i - c)^2/N$, met $c = n_{\text{syn}} / (n_{\text{syn}} + n_{\text{obs}})$

4. Vergelijk $pMSE$ met verwachte waarde onder een correct generatief model

# $pMSE$

Intuitief en flexibel, makkelijk te berekenen

Maar soms te simpel

Model specificatie kan lastig zijn

```{r}
S_pMSE <- synthpop::utility.tab.data.frame(
  data.frame(x = obs),
  data.frame(x = syn),
  vars = "x",
)$S_pMSE
```

De $pMSE$-ratio in ons voorbeeld: $`r round(S_pMSE, 3)`$

# Een density ratio raamwerk

Density ratios^[Zie _Masashi, Suzuki & Kanamori (2012). Density ratio estimation in machine learning._] als kwaliteitsmaat

<br>

$$r(x) = \frac{p(\boldsymbol{X}_{\text{syn }})}{p(\boldsymbol{X}_{obs})}$$
<br>
<br>

::: {.notes}
Laten we even teruggaan naar de observatie dat synthetische data hoge kwaliteit heeft, als de verdeling hetzelfde is als de verdeling van de geobserveerde data, oftewel als we de twee verdelingen niet kunnen onderscheiden.
Hoe kunnen we dat uitdrukken: als een ratio. Als deze ratio groot is, is er veel synthetische data in een regio waar weinig geobserveerde data is, en als deze klein is, hebben we een regio van de geobserveerde data niet voldoende zwaar gewogen in het genereren van de synthetische data. 
Dit kan je doen op een univariaat niveau, variabele voor variabele, maar deze ratio kan je ook in een keer schatten voor de multivariate verdelingen van de geobserveerde en gesynthetiseerde data. 
Deze density ratio zou je natuurlijk kunnen schatten door de kansverdelingen van de gesynthetiseerde en geobserveerde data los van elkaar te schatten, en vervolgens de ratio te nemen. 
Het nadeel hiervan is dat je schattingsfouten maakt bij beide kansverdelingen, en dat vervolgens de ratio nemen deze schattingsfouten onnodig vergroot. 
Onderzoek in dit veld heeft aangetoond dat je een nauwkeurigere schatting van de density ratio krijgt door deze direct te schatten. Hoe je dat kan doen kom ik later even op terug.
::: 

## Density ratios als utiliteitsmaat


```{r}
#| fig-align: center
library(patchwork)
library(ggplot2)



dlaplace <- function(x, mu = 0, sd = 1) exp(-abs(x-mu)/(sd / sqrt(2))) / (2*(sd / sqrt(2)))
dratio_lap_norm <- function(x, mu = 0, sd = 1) {
  dnorm(x, mu, sd) / dlaplace(x, mu, sd)
}

ggplot() +
  stat_function(fun = dlaplace, args = list(mu = 0, sd = 1),
                col = "#FDAE61", linewidth = 1, linetype = 1) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),
                col = "#F46D43", linewidth = 1, linetype = 4) +
  xlim(-5, 5) +
  ylim(0, 0.8) +
  ggdark::dark_mode() +
  ylab(NULL) +
ggplot() +
  stat_function(fun = dratio_lap_norm, args = list(mu = 0, sd = 1),
                linewidth = 1, linetype = 1, col = "#FEE08B") +
  xlim(-5, 5) +
  ylim(0, 2) +
  ggdark::dark_mode() +
  ylab(NULL) +
ggplot() +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),
                col = "#FDAE61", linewidth = 1, linetype = 1) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),
                col = "#F46D43", linewidth = 1, linetype = 4) +
  xlim(-5, 5) +
  ylim(0, 0.8) +
  ggdark::dark_mode() +
  ylab(NULL) +
ggplot() +
  geom_abline(intercept = 1, slope = 0, linewidth = 1, linetype = 1,
              col = "#FEE08B") +
  ggdark::dark_mode() +
  xlim(-5, 5) +
  ylim(0, 2) +
  ylab(NULL)
```

## Density ratios schatten

1. Schat de density ratio met een non-parametrische methode

- Geïmplementeerd in `R`-package [`densityratio`](https://github.com/thomvolker/densityratio).

2. Bereken een discrepantie maat voor de synthetische data (Kullback-Leibler divergence, Pearson divergence)

3. Vergelijk de Pearson divergence voor verschillende data sets

4. Optioneel: Toets de nulhypothese $p(\boldsymbol{X}_{\text{syn}}) = p(\boldsymbol{X}_{\text{obs}})$ d.m.v. een permutatietest.

## Density ratio in ons voorbeeld

:::: {.columns}

::: {.column width="60%"}

```{r}
library(densityratio)
```


```{r}
#| cache: true
#| echo: true

library(densityratio)
fit <- ulsif(syn, obs)
summary(fit, test = TRUE, parallel = TRUE)
```

:::

::: {.column width="40%"}

```{r}
#| fig-width: 5
ggplot(NULL, aes(x = syn, y = predict(fit))) +
  geom_point(col = "darkorange") +
  ggdark::dark_mode() +
  ylab(NULL)
```

:::

::::

## Density ratios voor multivariate synthetische data {.smaller}

### U.S. Current Population Survey (n = 5000)^[We zijn Jörg Drechsler dankbaar voor het delen van de data.]

- Vier continue variabelen (_leeftijd, inkomen, sociale uitkeringen, eigendomsbelasting_)
- Vier categorische variabelen (_geslacht, etniciteit, huwelijkse staat, opleidingsniveau_)

### Synthetische data modellen

(Multinomiale) logistische regressie voor categorische variabelen

1. Lineaire regressie
2. Lineaire regressie met transformaties (kubieke wortel)
3. Lineaire regressie met transformaties and semi-continu modelleren

## Utility of the synthetic data

![](files/syn-PEs.png)

# Terug naar de kern

## Samenvatting

<br>

Hoe specifieker hoe beter

<br>

Maar soms kan het niet specifiek, omdat je het doel niet kent

<br>

Dan blijft het belangrijk om met specifieke maten te evalueren

<br>

Maar kunnen algemene maten ook heel nuttig zijn

# Negeer niet de privacy risico's, synthetische data kan té realistisch zijn


# Dank voor jullie aandacht!

Vragen?

<br>
<br>

Nog meer vragen?

- [t.b.volker@uu.nl](mailto:t.b.volker@uu.nl)

## Kwaliteit van synthetische data punten

Voor iedere synthetische observatie wordt een density ratio waarde geschat

- Synthetische outliers detecteren / verwijderen

- Analyses op synthetische data herwegen


## Beschikbare extensies voor hoogdimensionele data

Aanname: subspace waarin de synthetische data goed gemodelleerd is, en een subspace waar de synthetische data niet goed gemodelleerd is

Doel: subspace herkennen waar de synthetische data niet goed gemodelleerd is, en hierop de density ratio schatten.

## Kruisvalidatie voor automatische parameter selectie

In alle bovengenoemde voorbeelden zijn dezelfde hyperparameters gebruikt

Kruisvalidatie zorgt ervoor dat de parameters in het density ratio model zo goed mogelijk gekozen worden.



# Bestaande kwaliteitsmaten als density ratios

## $pMSE$ {.smaller}

$$\begin{aligned}
r(\boldsymbol{X}) &= \frac{p(\boldsymbol{X}_{\text{syn}})}{p(\boldsymbol{X}_{\text{obs}})} \\
&= \frac{p(\boldsymbol{X} | Y = \text{synthetic})}{p(\boldsymbol{X}| Y = \text{observed})} 
= \frac{\frac{p(Y = \text{synthetic} | \boldsymbol{X})p(\boldsymbol{X})}{p(Y = \text{synthetic})}}{\frac{p(Y = \text{observed})p(\boldsymbol{X})}{p(Y = \text{observed})}} \\
&= \frac{p(Y = \text{observed})}{p(Y = \text{synthetic})} \frac{p(Y = \text{synthetic} | \boldsymbol{X})}{p(Y = \text{observed} | \boldsymbol{X})}
\end{aligned}$$

## Kullback-Leibler divergence {.smaller}

$$\begin{aligned}
KL(\boldsymbol{X}_{\text{syn}}, X_{\text{obs}}) = \int \log\Bigg(\frac{p(\boldsymbol{X}_{\text{syn}})}{p(\boldsymbol{X}_{\text{obs}})}\Bigg) p(\boldsymbol{X}_\text{syn})
\end{aligned}$$

Note that 
$$
\int \log\Bigg(\frac{p(\boldsymbol{X}_{\text{syn}})}{p(\boldsymbol{X}_{\text{obs}})}\Bigg) p(\boldsymbol{X}_\text{syn})
$$
can be approximated as
$$
\sum^{n_{\text{syn}}}_{i=1} \log(r(\boldsymbol{X}_\text{syn}))/n_{\text{syn}}.
$$
